# this is the default configuration if no overrides are specified.
defaults:
  - data: shakespeare
  - model: gpt_large
  - optimizer: adamw

# run specification - validation intervals and iterations, checkpointing and resuming.
# experiment - to test if I can override individual things in the base config.
model.pos_embedding: sin_cos

hooks:
  validate:
    validate_interval: 500
  checkpoint:
    checkpoint_interval: 500
  sample:
    sample_interval: 500
    tokens: 100

dataloading:
  batch_size: 32
  shuffle: True
run:
  # non integer epochs turns into some number of minibatches...
  epochs: 1

# if resuming from previous run.
resume:
  path: ../outputs/2024-07-04/16-46-52/checkpoints/final.pt

# shared contains parameters shared across model and data
shared:
  context_size: 128
  chunk_size: 2  # only valid for chunked model_type.

model_type: chunked  # or standard    !! maybe move this into model? !!

device: available # choice of available or set directly from torch options (see docs)
save_final: True

hydra:
  job:
    chdir: True
